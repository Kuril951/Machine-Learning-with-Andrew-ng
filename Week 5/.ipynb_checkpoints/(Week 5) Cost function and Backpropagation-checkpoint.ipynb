{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function and Backpropagation\n",
    "\n",
    "#### Neural network ( classification )\n",
    "\n",
    "![backpropagation](./img/backpropagation.png)\n",
    "\n",
    "- Training set : {$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})$}\n",
    "  - m : number of training set\n",
    "  - L : total number of layers in the network\n",
    "  - $S_l$ : number of units (not counting bias unit) in layer l  ( ex,  $S_1 = 3, S_2 = 5, S_3 = 5, S_4 = 4$ )\n",
    "  - K : number of output units/classes\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "- Logistic regression\n",
    "\n",
    "  - $J(\\theta)  = -{1 \\over m}[\\sum_{i = 1}^m y^{(i)}log(h_\\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\\theta(x^{(i)}))] + {\\lambda \\over 2m}\\sum_{j = 1}^n \\theta_j^2$ \n",
    "\n",
    "- Neural Network\n",
    "\n",
    "  - $J(\\theta)  = -{1 \\over m}[\\sum_{i = 1}^m\\sum_{k=1}^K y_k^{(i)}log((h_\\theta(x^{(i)}))_k) + (1 - y_k^{(i)})log(1 - (h_\\theta(x^{(i)}))_k)] + {\\lambda \\over 2m}\\sum_{l = 1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\theta_{j,i}^{(l)})^2$\n",
    "\n",
    "    <br>\n",
    "\n",
    "#### Backpropagation Algorithm\n",
    "\n",
    "- nerual network terminology for minimizing our cost function\n",
    "\n",
    "  \n",
    "\n",
    "- Inutuition : $\\delta_j^{(l)}  = $ \"error\" of node $j$ in layer $l$\n",
    "\n",
    "- For each output unit (layer L = 4)\n",
    "\n",
    "  - $\\delta_j^{(4)} = \\alpha_j^{(4)} - y_j  (vectorized : \\delta^{(4)} = \\alpha^{(4)} - y)$ \n",
    "  - $\\delta^{(3)} = (\\theta^{(3)})^T\\delta^{(4)} \\cdot g^\\prime(z^{(3)}) \\quad (g^\\prime(z^{(3)}) = \\alpha^{(3)} \\cdot (1 - \\alpha^{(3)}))$\n",
    "  - $\\delta^{(2)} = (\\theta^{(2)})^T\\delta^{(3)} \\cdot g^\\prime(z^{(2)})$\n",
    "  - No $\\delta^{(1)}$, because layer 1 is **input layer** which is not error\n",
    "  - ${\\partial \\over \\partial\\theta_{i, j}^{(l)}}J(\\theta) = \\alpha_j^{(l)}\\delta_j^{(l + 1)}$ (ignoring $\\lambda$ ; if $\\lambda$ = 0)\n",
    "\n",
    "- Algorithm\n",
    "\n",
    "  - Training set : {$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})$}\n",
    "  - set $\\Delta_{i, j}^{(l)} = 0$ (for all $i, j, l$) $\\quad$ (use to compute ${\\partial \\over \\partial\\theta_{i, j}^{(l)}}J(\\theta)$)\n",
    "  - For i = 1 to m\n",
    "    - set $\\alpha^{(i)} = x^{(i)}$\n",
    "    - perform forward propagation to compute $\\alpha^{(i)}$ for $l = 2, 3, ..., L$\n",
    "    - using $y^{(i)}$, compute $\\delta^{(L)} = \\alpha^{(L)} - y^{(i)}$\n",
    "    - compute $\\delta^{(L-1)}, \\delta^{(L-2)}, ..., \\delta^{(2)} \\qquad \\delta^{(l)} = ((\\theta^{(l)})^T\\delta^{(l+1)}) \\cdot \\alpha^{(l)} \\cdot (1 - \\alpha^{(l)})$\n",
    "    - $\\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)} + \\alpha_j^{(l)}\\delta_i^{(l+1)}$\n",
    "      - vectorized : $\\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(\\alpha^{(l)})^T$\n",
    "  - $D_{ij}^{(l)} := {1 \\over m}(\\Delta_{ij}^{(l)} + \\lambda\\theta_{ij}^{(l)}) \\quad \\mbox{if } j \\neq 0$\n",
    "  - $D_{ij}^{(l)} := {1 \\over m}\\Delta_{ij}^{(l)} \\quad \\mbox{if } j = 0$\n",
    "  - ${\\partial \\over \\partial\\theta_{i, j}^{(l)}}J(\\theta) = D_{ij}^{(l)}$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
